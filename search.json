[{"title":"spark—pivot 透视","url":"/2019/12/14/pivot/","content":"\n# spark学习之Pivot\n\n### 1，先看一下源数据\n\nscala代码，生成模拟数据。\n\n```scala\n    val data = Seq(\n      Row(\"20191214\" , \"产品3\" , 13 , 4),\n      Row(\"20191214\" , \"产品2\" , 10 , 3),\n      Row(\"20191214\" , \"产品3\" , 13 , 3),\n      Row(\"20191214\" , \"产品4\" , 12 , 2),\n      Row(\"20191213\" , \"产品1\" , 15 , 2),\n      Row(\"20191213\" , \"产品2\" , 10 , 3),\n      Row(\"20191213\" , \"产品3\" , 13 , 4),\n      Row(\"20191213\" , \"产品1\" , 15 , 1)\n    )\n\n    val df = spark.createDataFrame(\n      spark.sparkContext.parallelize(data , 1)\n      ,\n      StructType(\n        Array(\n          StructField(\"ymd\" , StringType),\n          StructField(\"item\" , StringType),\n          StructField(\"price\" , IntegerType),\n          StructField(\"cnt\" , IntegerType)\n        )\n      )\n    )\n\n\tdf.show()\n```\n\n结果：\n\n|     ymd| item|price|cnt|\n|--------|-----|-----|---|\n|20191214|产品3|   13|  4|\n|20191214|产品2|   10|  3|\n|20191214|产品3|   13|  3|\n|20191214|产品4|   12|  2|\n|20191213|产品1|   15|  2|\n|20191213|产品2|   10|  3|\n|20191213|产品3|   13|  4|\n|20191213|产品1|   15|  1|\n\n## 2，第一个透视\n\n```scala\n\tval df2 = df\n      .groupBy(\"ymd\" )\n      .pivot(\"item\")\n      .agg(sum(col(\"price\")*col(\"cnt\")))\n      .na.fill(0)\n      .orderBy(\"ymd\")\n      .drop(\"item\")\n```\n\n- 这段代码先按照 ymd 分组，然后透视了每组里面的 item 组织成了右边的列，然后聚合函数操作生成了dataframe。\n\n- 注意：\n  1. 先分组\n  2. 透视需要展开的列\n  3. 聚合函数操作的结果影响的是每一个单元格\n  \n- 简单说明：\n\n  1. 此功能按天查看了每个产品的总的流水\n\n     13号，只有产品1，2，3\n\n     14号，只有产品2，3，4\n\n     透视 item 后展开的列是全集，即产品1，2，3，4\n     \n  2. 聚合函数的功能是对 price列* cnt列的求积后求和\n  \n     注意观察，是对每个组中的不同的 pivot 的值的操作，相当于是groupby( ymd , item) \n> 结果1：\n\n|     ymd|产品1|产品2|产品3|产品4|\n|--------|-----|-----|-----|-----|\n|20191213|   45|   30|   52|    0|\n|20191214|    0|   30|   91|   24|\n\n---\n\n### 结合下面的代码在体会一下 pivot 和 groupby 的关系\n\n```scala\n\tval df3 = df.groupBy(\"ymd\" , \"item\")\n        .pivot(\"item\")\n        .agg(sum(col(\"price\")*col(\"cnt\")))\n        .na.fill(0)\n        .orderBy(\"ymd\" , \"item\")\n        .drop(\"ymd\")\n    df3.show()\n```\n\n> 上面的代码直接按照 ymd 和 item 进行分组，然后 pivot 之后聚合。\n>\n> 结果2：\n\n| item|产品1|产品2|产品3|产品4|\n|-----|-----|-----|-----|-----|\n|20191213|   45|    0|    0|    0|\n|20191213|    0|   30|    0|    0|\n|20191213|    0|    0|   52|    0|\n|20191214|    0|   30|    0|    0|\n|20191214|    0|    0|   91|    0|\n|20191214|    0|    0|    0|   24|\n\n**注意观察上面的结果1 和 2，计算的数值一样，但是 2 的计算过程中是按照两列分组，而 1 是 ymd 一列分组，所以记录数不一样，对应后面的透视的聚合点（单元格）位置也就不一样。**\n\n## 2，简单练习\n\n按照产品分别查看不同日期的流水\n\n```scala\n\tval df4 = df\n      .groupBy( \"item\")\n      .pivot(\"ymd\")\n      .agg(sum(col(\"price\")*col(\"cnt\")))\n      .na.fill(0)\n      .orderBy( \"item\")\n      .drop(\"ymd\")\n\tdf4.show()\n```\n\n> 结果3：\n\n| item|20191213|20191214|\n|-----|--------|--------|\n|产品1|      45|       0|\n|产品2|      30|      30|\n|产品3|      52|      91|\n|产品4|       0|      24|\n\n## 3，反透视\n\n透视本质上是将不同行的信息转化到列上了，同时支持了聚合操作。那么如果想要从列的信息返回行级的信息呢？就需要用到反透视了。\n\n**已上面的 df2 来演示反透视。**\n\n```scala\ndef unpivot(df:DataFrame , groupCol:String , v1Col:String , v2Col:String):DataFrame={\n    val cols = df.columns\n    var sql = s\"stack(${cols.length-1} \"\n    cols.foreach(x=>{\n      if (!x.equals(groupCol)){\n        sql += (s\", '${x}' ,`${x}`\")\n      }\n    })\n    sql += (s\") as (`${v1Col}` , `${v2Col}`)\")\n    df.selectExpr(groupCol , sql)\n  }\nunpivot(df2 , \"ymd\" , \"item\" , \"ls\").show()\n```\n> 结果4：\n>\n> |     ymd| item| ls|\n> |--------|-----|---|\n> |20191213|产品1| 45|\n> |20191213|产品2| 30|\n> |20191213|产品3| 52|\n> |20191213|产品4|  0|\n> |20191214|产品1|  0|\n> |20191214|产品2| 30|\n> |20191214|产品3| 91|\n> |20191214|产品4| 24|\n\n\n- 上面的反透视采用了 sql 的写法。\n- 观察后思考不难得出先透视后反的话是无法保证数据的复原的，除非透视聚合的时候完整保留信息。\n- 透视和反透视应用广泛，但是对之特别多的时候慎重，以免列过多造成卡死。","tags":["scala","透视","pivot","spark"],"categories":["pivot","spark"]},{"title":"git+hexo博客","url":"/2019/12/12/Gitpage+hexo制作博客/","content":"\n# Hexo+Gitpage博客教程\n\n## 1，[注册Github](https://github.com/)\n\ngithub不多说了，网上关于git博客制作需要注意的教程和资料很多，新建一个仓库仓库地址为用户名.github.io这个仓库需要公开等，详情搜索Gitpage。注意本地最好安装有gitbash。\n\n## 2，[下载NodeJS](https://nodejs.org/en/download/)\n\n**一定要下载nodejs** 这是重点，下载安装也不多说了，地址在标题上。\n\n## 3，登陆[Hexo](https://hexo.io/zh-cn/)\n\n**--任意目录下执行 npm install hexo-cli -g （最好在gitbash命令行中）用于安装hexo--**\n\n1，到一个以后准备当作博客根据地的目录下\n\n2，进入gitbash\n\n3，执行【hexo init .】，会将 blog 文件夹初始化成为 hexo的博客文件夹。此时目录下会有很多文件夹和文件，重点说一下source,themes,_config.yml\n\n​\t**source : **之后写博客的目录。\n\n​\t**themes : **下载的博客风格的目录。\n\n​\t**_config.yml : **这个文件是整个博客的配置信息所在。\n\n4，执行【npm install】多出了node_modules文件夹，是与package.json相关的用处，与依赖相关。\n\n5，执行【hexo server】开启hexo服务。\n\n关于hexo的详细信息请[参考](https://hexo.io/zh-cn/docs/)\n\n## 4，[hexo](https://hexo.io/themes/)选择风格\n\n在这个页面中点击图片会跳转预览，点击风格名会跳转到git地址。在git中一般会有关于这个风格的部署和配置的介绍。然后需要git clone所需要的风格项目。一般按照里面的配置就好了。\n\n## 5，hexo常用命令\n\n**hexo g : **生成页面\n\n**hexo d : **部署到git上\n\n**hexo s : **本地模拟服务\n\n","tags":["git","hexo"],"categories":["博客"]},{"title":"初识scala","url":"/2019/12/09/Scala基本语法/","content":"\n## Scala\n\n### 下载安装scala[去官网下载](https://www.scala-lang.org/)\n\n具体安装细节不做解释网上教程挺多的。笔者是使用idea编辑器下载scala支持插件开发学习scala。\n\n### scala语言的HelloWorld\n\n```scala\nobject First {\n\tdef main(args: Array[String]){\n        println(\"Hello world,scala\")\n    }\n}\n```\n\n直接粘贴进入idea即可运行。\n\n### 代码结构初识\n\n```scala\nobject First {\n}\n```\n\n暂时先理解scala中的object是一个完全静态的类所有方法变量全是静态的，class只能是实体类型不能静态化。以后再详细说明object和class的区别。\n\n```scala\ndef main(args: Array[String]){\n}\n```\n\n在scala中 **def**  表示声明一个方法，因为在object中所以方法是静态的。学过java的都知道静态的main函数自然是主函数了。再看主函数参数在java中  **String[] args**在这里写成了**args:Array[String]**。原因在于scala中是将变量的类型写在变量名后面的中间用 ：分开。而Array是scala中数组的表示，到了scala集合部分在细谈。\n\n```\nprintln(\"Hello world,scala\")\n```\n\nprintln是scala中默认环境的函数，不用声明类（似于java中的lang包），用于标准输出，对应的还有printf。\n\n","tags":["scala","java","大数据"],"categories":["scala"]}]